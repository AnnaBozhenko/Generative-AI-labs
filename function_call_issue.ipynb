{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, FunctionDeclaration, Tool, Content, Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_PROJECT_ID = \"enduring-coil-441309-i5\"\n",
    "LOCATION = \"us-central1\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up database schema and populate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql sqlite:///sample.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "create table if not exists products (\n",
    "    product_id integer primary key autoincrement,\n",
    "    product_name varchar(255) not null,\n",
    "    price decimal(10, 2) not null\n",
    ");\n",
    "\n",
    "create table if not exists staff (\n",
    "    staff_id integer primary key autoincrement,\n",
    "    first_name varchar(255) not null,\n",
    "    last_name varchar(255) not null\n",
    ");\n",
    "\n",
    "create table if not exists orders (\n",
    "    order_id integer primary key autoincrement,\n",
    "    customer_name varchar(255) not null,\n",
    "    staff_id integer not null,\n",
    "    product_id integer not null,\n",
    "    foreign key (staff_id) references staff (staff_id),\n",
    "    foreign key (product_id) references products (product_id)\n",
    ");\n",
    "\n",
    "-- populate\n",
    "\n",
    "insert into products(product_name, price) values\n",
    "  \t('Laptop', 799.99),\n",
    "  \t('Keyboard', 129.99),\n",
    "  \t('Mouse', 29.99);\n",
    "\n",
    "INSERT INTO orders (customer_name, staff_id, product_id) VALUES\n",
    "  \t('David Lee', 1, 1),\n",
    "  \t('Emily Chen', 2, 2),\n",
    "  \t('Frank Brown', 1, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_file = \"sample.db\"\n",
    "\n",
    "db_conn = sqlite3.connect(db_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create API functions, calling database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tables() -> list[str]:\n",
    "    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n",
    "    cursor = db_conn.cursor()\n",
    "    cursor.execute(\"select * from sqlite_master where type = 'table';\")\n",
    "    table = cursor.fetchall()\n",
    "    return [t[0] for t in table]\n",
    "\n",
    "\n",
    "def describe_table(table_name: str) -> list[tuple[str, str]]:\n",
    "    \"\"\"Look up the table schema. Returns list of column names, where each entry is a tuple of (column, type).\"\"\"\n",
    "    cursor = db_conn.cursor()\n",
    "    cursor.execute(f\"pragma table_info({table_name});\")\n",
    "    schema = cursor.fetchall()\n",
    "    return [(col[1], col[2]) for col in schema]\n",
    "\n",
    "\n",
    "def execute_query(sql: str) -> list[list[str]]:\n",
    "    \"\"\"Execute a SELECT statement, returning the results.\"\"\"\n",
    "    cursor = db_conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    return cursor.fetchall()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tables_func = FunctionDeclaration(\n",
    "    name=\"list_tables\",\n",
    "    description=\"Retrieve the names of all tables in the database.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {}\n",
    "        },\n",
    ")\n",
    "\n",
    "describe_table_func = FunctionDeclaration(\n",
    "    name=\"describe_table\",\n",
    "    description=\"Look up the table schema. Returns list of columns, where each entry is a tuple of (column, type).\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"table_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Name of the table.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"table_name\"\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n",
    "execute_query_func = FunctionDeclaration(\n",
    "    name=\"execute_query\",\n",
    "    description=\"Execute a SELECT statement, returning the results.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"sql\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"SQL query to the database.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"sql\"\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "list_tables_func = FunctionDeclaration.from_func(list_tables)\n",
    "describe_table_func = FunctionDeclaration.from_func(describe_table)\n",
    "execute_query_func = FunctionDeclaration.from_func(execute_query)\n",
    "\n",
    "db_tool = vertexai.generative_models.Tool(\n",
    "    function_declarations = [list_tables_func, describe_table_func, execute_query_func]\n",
    ")\n",
    "\n",
    "instruction = \"\"\"You are a helpful chatbot that can interact with an SQL database for a computer\n",
    "store. You will take the users questions and turn them into SQL queries using the tools\n",
    "available. Once you have the information you need, you will answer the user's question using\n",
    "the data returned. Use list_tables to see what tables are present, describe_table to understand\n",
    "the schema, and execute_query to issue an SQL SELECT query.\"\"\"\n",
    "\n",
    "vertexai.init(project=GOOGLE_PROJECT_ID,\n",
    "              location=LOCATION)\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\",\n",
    "                        tools = [db_tool],\n",
    "                        system_instruction = instruction)\n",
    "\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the most expensive product?\" \n",
    "\n",
    "resp = chat.send_message(content=query,\n",
    "                         tools=[db_tool])\n",
    "function_calls = resp.candidates[0].function_calls\n",
    "func = function_calls[-1] # the last query is a clue to retrieve user's answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      function_call {\n",
       "        name: \"list_tables\"\n",
       "        args {\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    parts {\n",
       "      text: \"\\n\\n\"\n",
       "    }\n",
       "    parts {\n",
       "      function_call {\n",
       "        name: \"describe_table\"\n",
       "        args {\n",
       "          fields {\n",
       "            key: \"table_name\"\n",
       "            value {\n",
       "              string_value: \"products\"\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    parts {\n",
       "      text: \"\\n\\n\"\n",
       "    }\n",
       "    parts {\n",
       "      function_call {\n",
       "        name: \"execute_query\"\n",
       "        args {\n",
       "          fields {\n",
       "            key: \"sql\"\n",
       "            value {\n",
       "              string_value: \"SELECT name FROM products ORDER BY price DESC LIMIT 1\"\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HATE_SPEECH\n",
       "    probability: NEGLIGIBLE\n",
       "    probability_score: 0.171875\n",
       "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
       "    severity_score: 0.111328125\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "    probability: NEGLIGIBLE\n",
       "    probability_score: 0.2578125\n",
       "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
       "    severity_score: 0.188476562\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HARASSMENT\n",
       "    probability: NEGLIGIBLE\n",
       "    probability_score: 0.198242188\n",
       "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
       "    severity_score: 0.104980469\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "    probability: NEGLIGIBLE\n",
       "    probability_score: 0.103515625\n",
       "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
       "    severity_score: 0.078125\n",
       "  }\n",
       "  avg_logprobs: -0.041282428635491267\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 243\n",
       "  candidates_token_count: 27\n",
       "  total_token_count: 270\n",
       "}\n",
       "model_version: \"gemini-1.5-flash-001\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT product_name FROM products ORDER BY price DESC LIMIT 1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_evaluation = func.args[\"sql\"].replace(\"name\", \"product_name\")\n",
    "func_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Laptop',)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_response = execute_query(func_evaluation)\n",
    "sql_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Laptop'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_response = {\"text\": \"; \".join([\", \".join(items) for items in sql_response])}\n",
    "api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 Please ensure that the number of function response parts should be equal to number of function call parts of the function call turn.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\grpc\\_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1175\u001b[0m (\n\u001b[0;32m   1176\u001b[0m     state,\n\u001b[0;32m   1177\u001b[0m     call,\n\u001b[0;32m   1178\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(\n\u001b[0;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1180\u001b[0m )\n\u001b[1;32m-> 1181\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\grpc\\_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Please ensure that the number of function response parts should be equal to number of function call parts of the function call turn.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.75.10:443 {created_time:\"2024-11-14T09:05:21.4282646+00:00\", grpc_status:3, grpc_message:\"Please ensure that the number of function response parts should be equal to number of function call parts of the function call turn.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_response \u001b[39m=\u001b[39m chat\u001b[39m.\u001b[39;49msend_message(\n\u001b[0;32m      2\u001b[0m     Content(\n\u001b[0;32m      3\u001b[0m             role \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m             parts \u001b[39m=\u001b[39;49m [Part\u001b[39m.\u001b[39;49mfrom_function_response(name \u001b[39m=\u001b[39;49m func\u001b[39m.\u001b[39;49mname, response \u001b[39m=\u001b[39;49m {\u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: api_response})]\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:1257\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[1;34m(self, content, generation_config, safety_settings, tools, labels, stream)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_message_streaming(\n\u001b[0;32m   1250\u001b[0m         content\u001b[39m=\u001b[39mcontent,\n\u001b[0;32m   1251\u001b[0m         generation_config\u001b[39m=\u001b[39mgeneration_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m         labels\u001b[39m=\u001b[39mlabels,\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_message(\n\u001b[0;32m   1258\u001b[0m         content\u001b[39m=\u001b[39;49mcontent,\n\u001b[0;32m   1259\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[0;32m   1260\u001b[0m         safety_settings\u001b[39m=\u001b[39;49msafety_settings,\n\u001b[0;32m   1261\u001b[0m         tools\u001b[39m=\u001b[39;49mtools,\n\u001b[0;32m   1262\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1263\u001b[0m     )\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:1386\u001b[0m, in \u001b[0;36mChatSession._send_message\u001b[1;34m(self, content, generation_config, safety_settings, tools, labels)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     request_history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_history \u001b[39m+\u001b[39m history_delta\n\u001b[1;32m-> 1386\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49m_generate_content(\n\u001b[0;32m   1387\u001b[0m         contents\u001b[39m=\u001b[39;49mrequest_history,\n\u001b[0;32m   1388\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[0;32m   1389\u001b[0m         safety_settings\u001b[39m=\u001b[39;49msafety_settings,\n\u001b[0;32m   1390\u001b[0m         tools\u001b[39m=\u001b[39;49mtools,\n\u001b[0;32m   1391\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1392\u001b[0m     )\n\u001b[0;32m   1393\u001b[0m     \u001b[39m# By default we're not adding incomplete interactions to history.\u001b[39;00m\n\u001b[0;32m   1394\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_validator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:779\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generates content.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \n\u001b[0;32m    754\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[39m    A single GenerationResponse object\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    771\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(\n\u001b[0;32m    772\u001b[0m     contents\u001b[39m=\u001b[39mcontents,\n\u001b[0;32m    773\u001b[0m     generation_config\u001b[39m=\u001b[39mgeneration_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    777\u001b[0m     labels\u001b[39m=\u001b[39mlabels,\n\u001b[0;32m    778\u001b[0m )\n\u001b[1;32m--> 779\u001b[0m gapic_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prediction_client\u001b[39m.\u001b[39;49mgenerate_content(request\u001b[39m=\u001b[39;49mrequest)\n\u001b[0;32m    780\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_response(gapic_response)\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\google\\cloud\\aiplatform_v1\\services\\prediction_service\\client.py:2159\u001b[0m, in \u001b[0;36mPredictionServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   2156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m   2158\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 2159\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[0;32m   2160\u001b[0m     request,\n\u001b[0;32m   2161\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[0;32m   2162\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   2163\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[0;32m   2164\u001b[0m )\n\u001b[0;32m   2166\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[0;32m   2167\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Kaggle\\GenAI-5-day\\env\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 Please ensure that the number of function response parts should be equal to number of function call parts of the function call turn."
     ]
    }
   ],
   "source": [
    "user_response = chat.send_message(\n",
    "    Content(\n",
    "            role = \"user\",\n",
    "            parts = [Part.from_function_response(name = func.name, response = {\"content\": api_response})]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
